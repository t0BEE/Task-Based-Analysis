\section{Related Work}

\subsection{Tasking}
  Having the task construct as mean to parallelize a program allows to utilize irregular parallelism.
  Irregular parallelism is widely used these days and includes parallel execution of while loops or recursive function calls for instance.
  Tasking explicitly specifies independent units of work which can be executed in parallel.
  This enables a more dynamic way of parallelization.
  However, a disadvantage of this kind of parallelism is that it increases the complexity of a program and poor implementations can lead to an increased overhead.~\cite{Ayguade.2009}~\cite{LaGrone.2011}
  
  
  The used runtime system plays a crucial role when it comes to the application's performance using tasking. 
  For example, as the tasking construct creates several tasks which will be executed by threads, the performance of an application relies on the thread scheduler of the runtime system.
  The authors of \cite{LaGrone.2011} discuss different aspects of a runtime system using the task construct.
  The efficiency of such depends on the data structures which store unfinished tasks, manage task switching and regulate task creation.
  Additionally, the data structures to organize task synchronization and manage the memory footprint of a task are also important.
  The ideal way of work of the thread scheduler, for example, is to maximize concurrency, load balancing and data locality.
  This can be achieved by different ways to manage queue storing all tasks ready to be executed.
  Furthermore, the scheduler can be \textit{depth-first} or \textit{breadth-first}.
  Breadth-first means that child tasks created by the parent task and put into the queue to be scheduled at any time.
  Whereas breadth-first schedulers switch to child tasks directly after their creation and execute them.
  This leads to a smaller number of tasks in the queue and less concurrency opportunities.
  However, the data reuse increases.
 
 	
  The following subsections \ref{subsec:OpenMP} and \ref{subsec:HPX} introduce two runtime systems utilizing tasks.
  These will be compared in this work.


\subsection{OpenMP}
\label{subsec:OpenMP}

%1\cite{Ayguade.2009}
%2\cite{LaGrone.2011} --> some further information and alternatives can be found here
%3\cite{MKlemm.2018}
%4\cite{Qawasmeh.2014}
%5\cite{Duran.2008}
%6\cite{Furlinger.2009}

% example implementation here - show the directives

  OpenMP includes the tasking model since version 3.0.
  Various directives for that purpose are being included since to specify and manage  tasks.
  However, OpenMP still differs between explicit and implicit tasks.
  Implicit tasks are created by parallel regions as side effect.
  The programmer does not need to specify or know about them.
  Explicit tasks on the other hand are defined by the programmer using the task directives.
  The simplest directive to define an explicit task is \texttt{omp task} and the enclosed code area is the task region.
  These are executed by any task in the current team whenever they are ready.~\cite{Ayguade.2009}~\cite{LaGrone.2011} 

  Like in parallel regions variable scopes can be defined by \texttt{shared}, \texttt{private} or \texttt{firstprivate} to enable shared variables between these tasks.
  


  1,2- by default, a thread is tied to a task as it begins to execute the task
    1,2- this means that the task is only executed by this thread, however the thread can still execute other tasks in case the task reaches a task scheduling point and has to suspend its work
    1,2- this restriction can be avoided by defining the tasks untied
    
  5- when a thread encounters a task directive, the data environment is captured
  		--> the environment and the code block constitute the generated task


  4- OpenMP runtime becomes responsible for the scheduling of tasks
  4- task construct allows developers to dynamically create asynchronous units of work
  2- explicit scheduling can be achieved by 
      - \texttt{taskwait}, suspend encountering task region and wait for all child tasks to complete
      - \texttt{barrier}, wait till all reach the barrier

  6- tasks result in more dynamic and unpredictable execution characteristics
  	--> does the thread switch to the child task or not ? --> use true false at the creation to minimize that uncertainty


\subsection{HPX}
\label{subsec:HPX}
\subsubsection{HPX}
  \cite{Kaiser.2014}
  - general purpose C++ runtime system for parallel and distributed applications of any scale
  - innovative mixture of global system-wide address space, fine grain parallelism, and lightweight synchronization
    + implicit message driven computation
    + semantic equivalent of local and remote execution
    + explicit support for accelerators such as GPGPUs
  - aims to resolve scalability / resiliency / power efficiency / runtime adaptive resource management in the evolve from Peta- to Exascale systems
  
\cite{Kaiser.2009}
  - introduce a new model of parallel execution --> Parallex
    - efficiency / scalability
    
  - HPX = run time system supporting the ParalleX model
  
--> folgende sind groÃŸe Vergleiche zu MPI ---
    - dynamically schedule multiple threads moving the work to the data
     -> instead of statically designated processes synchroized by message passing
    - use local synchronization and global asynchronicity (MPI uses messages as synchronization)
    - Global barriers are essentially eliminated and instead replaced by
lightweight Local Control Objects (LCOs) -> futures / mutexes
    - key to efficiency and latency hiding is the message-driven work-queue
	  -> apply user tasks to physical processing resources
	- uses active global address space (AGAS)
	  - allows to move virtual objects in physical space without changing virtual names
	  ---
	- conventiionally 1 process = 1 processor
	  -> ParalleX, use \textit{parallel processes}: map a process to multiple cores, allow many concurrent threads and child processes
	- use a thread manager implemented as FCFS scheduler 
	  - OS threads work from a single queue of tasks 
	  --> sufficient for small amount of concurrent OS threads
	  --> there is work on a more scalable , work stealing scheduler using one queue per OS thread (core)
	  

\subsubsection{Tasks in HPX}
\cite{TheSTEARGroup.2020}
  - HPX allows the same tasking method as the C++ standard library --> using async and futures
    - lightweight threads are used which have faster context switches and smaller stacks
    - async: launches a task asynchronously
    - future<T>: represents a value of type T which will be available in the future
    - continuation - then: take a function to run after the task is done

  - future: acts as a placeholder for a result not known yet
    --> the computation for that value is not yet completed
    --> using the get() method blocks the calling thread and it passively waits until the value of the future is available
    - when a future is created it spawns a new HPX-Thread and places it on the thread queue


\subsection{hpxMP}
\cite{TianyiZhang.2019}
- introduction of hpxMP

\cite{Zhang.2192020}
- OpenMP 5 tasks in hpxMP


    
\subsection{Benchmark Algorithm}
- Barcelona OpenMP Task Suite (BOTS)
	- aim of the suite is to provide a collection of benchmarks that would allow vendors to test the impact of different implementation decisions in a multicore architecture
	- 9 benchmarks
		> Alignment: align protein sequences against every other sequence using a special algorithm
		> FFT: Fast Fourier Transformation
		> Fibonacci: calculation of the nth number in the Fibonacci sequence
		> Floorplan: compute the optimal floorplan distribution of a number of cells
		> Health: simulate the Columbian Health Care System
		> N Queens: the n-queen problem, to find placements of queens on a chessboard under special conditions
		> Sort: parallel execution of merge sort
		> SparseLU: LU matrix factorization over sparse matrices
		> Strassen: hierarchical decomposition of a matrix for multiplication of large dense matrices
	--> different version of each benchmark and some with cut-off version to avoid high amount of tasks
	

	- Fibonacci --> high amount of tasks, having the scheduling as the main part of the workload
	- MergeSort --> easy to implement, good use case and high number of tasks
	
