\section{Related Work}

\subsection{Tasking}
\label{subsec:Tasking}
  Having the task construct as mean to parallelize a program allows to utilize irregular parallelism.
  Irregular parallelism is widely used these days and includes parallel execution of while loops or recursive function calls for instance.
  Tasking explicitly specifies independent units of work which can be executed in parallel.
  This enables a more dynamic way of parallelization.
  However, a disadvantage of this kind of parallelism is that it increases the complexity of a program and poor implementations can lead to an increased overhead.~\cite{Ayguade.2009}~\cite{LaGrone.2011}
  
  
  The used runtime system plays a crucial role when it comes to the application's performance using tasking. 
  For example, as the tasking construct creates several tasks which will be executed by threads, the performance of an application relies on the thread scheduler of the runtime system.
  The authors of \cite{LaGrone.2011} discuss different aspects of a runtime system using the task construct.
  The efficiency of such depends on the data structures which store unfinished tasks, manage task switching and regulate task creation.
  Additionally, the data structures to organize task synchronization and manage the memory footprint of a task are also important.
  The ideal way of work of the thread scheduler, for example, is to maximize concurrency, load balancing and data locality.
  This can be achieved by different ways to manage queue storing all tasks ready to be executed.
  Furthermore, the scheduler can be \textit{depth-first} or \textit{breadth-first}.
  Breadth-first means that child tasks created by the parent task and put into the queue to be scheduled at any time.
  Whereas breadth-first schedulers switch to child tasks directly after their creation and execute them.
  This leads to a smaller number of tasks in the queue and less concurrency opportunities.
  However, the data reuse increases.
 
 	
  The following subsections \ref{subsec:OpenMP} and \ref{subsec:HPX} introduce two runtime systems utilizing tasks.
  These will be compared in this work.


\subsection{Tasks in OpenMP}
\label{subsec:OpenMP}
  OpenMP includes the tasking model since version 3.0.
  Various directives for that purpose are being included since to specify and manage  tasks.
  However, OpenMP still differs between explicit and implicit tasks.
  Implicit tasks are created by parallel regions as side effect.
  The programmer does not need to specify or know about them.
  Explicit tasks on the other hand are defined by the programmer using the task directives.
  The simplest directive to define an explicit task is \texttt{omp task} and the enclosed code area is the task region.
  These are executed by any task in the current team whenever they are ready.~\cite{Ayguade.2009}~\cite{LaGrone.2011}

%MAYBE example implementation here - show the directives

  When a thread encounters a task directive the current data environment is captured.
  This environment and the block of code in the task region form the generated task.
  The variable scopes of a task region can be defined like in parallel regions.
  By adding the clauses \texttt{shared}, \texttt{private} or \texttt{firstprivate} to the task directive the declared variables are either shared among tasks or not.~\cite{Duran.2008}
  
  
  By default a thread is tied to a task as it begins to execute it.
  This means that this task is only allowed to be executed by one thread.
  However this thread can still execute other tasks in case it reaches task scheduling points and switches the execution to another task.
  The suspended task is put into the queue and has to wait until its tied thread is ready to continue its execution.
  This restriction can be avoided by defining the task \texttt{untied}.
  By doing so, load balancing is increased, but data locality is decreased.~\cite{Ayguade.2009}~\cite{LaGrone.2011}


  Tasking results in a more dynamic execution and might be unpredictable without explicit scheduling.
  For example the runtime system has either a depth- or breadth-first scheduler.
  Meaning that it either switches to the child task after creation or finishes the parent task first.
  The used OpenMP runtime system is responsible to decide.
  Explicit scheduling can be used to avoid this uncertainty.
  For instance, adding an \texttt{if}-clause to the task directive and evaluating it to false makes the encountering thread to suspend its current work and execute the child task immediately.
  Furthermore, explicit task scheduling is possible by using the \texttt{taskwait} and \texttt{barrier} directives.
  \texttt{taskwait} suspends the encountering task region and forces it to wait for all child tasks to complete.
  Additionally \texttt{taskgroup} defines a task region which will be executed and at the end the current task has to wait for all its child and their descendants.~\cite{Qawasmeh.2014}~\cite{Furlinger.2009}


\subsection{HPX}
\label{subsec:HPX}
\subsubsection{HPX}
  %1\cite{Kaiser.2014}
  %2\cite{Kaiser.2009}	  
  
  HPX aims to resolve problems in scalabilitiy, power efficiency, resource management and even further ones which may raise and increase by moving from Peta- to Exascale systems.
  It focuses on parallel and distributed development independent of the system's scale.
  This is done by providing a general purpose C++ runtime system with an innovative design.
  It is described as a mixture of lightweight synchronization, global system-wide address space and fine grain parallelism.
  Message driven computation can be achieved with a small amount of effort.
  A remote execution has the same semantics as a local one and can thereby be done implicitly.
  Furthermore, it offers the programmer explicit support for accelerators such as general purpose graphical processing units.~\cite{Kaiser.2014}
  
  
  A new model of parallel execution is introduced to enable all of this features --- called \textit{Parallex}.
  HPX is the runtime system supporting this new model.
  The authors of \cite{Kaiser.2009} compare HPX with MPI extensively in their work.
  For example, ParalleX maps a process to multiple cores in contrast to the conventional way mapping a process to just one core.
  Thereby many concurrent threads inside this process are allowed and it is possible  to create child process accessing the cores.
    
  % Possible to explain ParalleX even further --> architecture e.g. (1 and 2)
	
	
\subsubsection{Tasks in HPX}
  Various program models increase their efforts on tasking in recent years and increase the possibilities in local parallelism.
  However, in contrast to the other program models, HPX offers a way for homogeneous execution of local and remote operations.
  This is achieved by using the task model combined with a so called \textit{Active Global Address Space}.
  As all nodes of an application share a common address space, objects and tasks can be migrated across nodes.
  Instead of moving the data to nodes the tasks can be moved to the data, which allows faster communication as the instructions are often smaller in size than the actual data.
  Furthermore the migration can be done without changing the global address as all nodes share the address space.
  Additional work is saved due to this fact.~\cite{Kaiser.2014}
  
  Another advantage of HPX which also makes it convenient to C++/C programmers is that it uses the same tasking methods as the C++ standard.
  No new semantic or syntax is used.
  However, the actual standard is extended to support remote operations.
  To use tasking in HPX, tasks have to be defined by \texttt{async} and \texttt{future}.
  \texttt{async} launches a task asynchronously.
  \texttt{future<T>} represents a value of type \texttt{T} which will be available in the future, when the execution of a task finishes.
  It acts as a placeholder of a result not known yet and when created spawns a new HPX-Thread which is placed on the thread queue.
  In case another thread wants to access a future it is blocked and has to wait until the value of the future is available.
  Using \texttt{get()} is an explicit option for the programmer to let the thread wait for the future.~\cite{Kaiser.2014}~\cite{TheSTEARGroup.2020}
  
  
  The HPX thread scheduling system uses lightweight threads which allow faster context switches and smaller stacks.
  Task migration is therefore more efficient as less data has to be moved.
  Furthermore, it is designed to handle millions of tasks efficiently.
  In \cite{Kaiser.2014} HPX tasks are compared to OpenMP tasks for their GFLOPS on various numbers of cores.
  However the HTTS benchmark is used which uses no-op tasks and can therefore only compare the thread scheduling performance without actual workload.
  One reason why HPX performed better in the HTTS benchmark is that it uses constraint based synchronization instead of global barriers, like in OpenMP.
  For example, OpenMP uses implicit barriers at the end of each loop.
  In contrast to that HPX allows to execute the code following the loop in case no data dependencies are given.
  
  
\subsection{hpxMP}
  A rather new approach is \textit{hpxMP}.
  It uses OpenMP 5 tasks together with HPX.  
  
  \textit{\textbf{TODO!}}
%\cite{TianyiZhang.2019}
%- introduction of hpxMP

%\cite{Zhang.2192020}
%- OpenMP 5 tasks in hpxMP


    
\subsection{Benchmark Algorithm}
\label{subsec:BOTS}
  The authors of \cite{Duran.2009} introduce a benchmark suite to test the impact of different implementation decisions using OpenMP tasks.
  The benchmark suite is called \textit{Barcelona OpenMP Task Suite (BOTS)} and includes nine benchmarks:
  \begin{description}
    \item[Alignment] is an algorithm aligning protein sequences against every other sequence.
    \item[Fast Fourier Transformation]
    \item[Fibonacci] calculates the \(n^{th}\) number of the Fibonacci sequence.
    \item[Floorplan] computes the optimal floorplan distribution of a number of cells.
    \item[Health] simulates the Colombian Health Care System
    \item[N Queens] tries to find placements of \(n\) queens on a chessboard under special condition
    \item[Sort] is a special kind of parallel merge sort execution
    \item[SparseLU] calculates a LU matrix factorization over sparse matrices
    \item[Strassen] hierarchically decomposes a matrix for multiplication of large dense matrices
  \end{description}
  The suite offers different versions of each benchmark, e.g. a version of Fibonacci using a cut-off to avoid an high amount of tasks.
 
