\section{Implementation \& Design}
  For this paper the Fibonacci and the Sort algorithm from BOTS are chosen to be implemented, as introduced in subsection \ref{subsec:BOTS}.
  Additionally, a generic algorithm is implemented.
  All three are implemented using OpenMP, HPX and in a sequential way.
  
  \paragraph{Fibonacci}
  The algorithm can spawn a high amount of tasks with small workload.
  The workload of forking and joining new threads might have the biggest impact when it comes to performance.
  
  The implementation is done without cut offs for the HPX and OpenMP versions.
  This means that every recursive call of the Fibonacci function creates a new task.
  \begin{lstlisting}[
  caption=Fibonacci OpenMP Implementation,
  label=lst1:fibOMP,
]
long long fibonacci(long long input) {
    if (input < 2 ) return input;
    long long x, y;
    #pragma omp task shared(x) firstprivate(input)
    x = fibonacci(input - 1);
    #pragma omp task shared(y) firstprivate(input)
    y = fibonacci(input - 2);
    #pragma omp taskwait
    return x + y;
}
\end{lstlisting}
  Listing \ref{lst1:fibOMP} shows a first implementation of the Fibonacci algorithm using OpenMP.
  It shows which directives are used.
  As explained in subsection \ref{subsec:OpenMP}, the \texttt{\#pragma omp task} directive creates a new task which may be executed.
  Using the \texttt{shared} and \texttt{firstprivate} clauses adjusts the variable scopes so that each task has its own instance of the parameter input.
  Variables \texttt{x} and \texttt{y} are shared which means no new instance is created.
  At the end the \texttt{\#pragma omp taskwait} directive tells the function to wait until all the child tasks finished their execution.
  
\begin{lstlisting}[
  caption=Fibonacci HPX Implementation,
  label=lst1:fibHPX,
]
long long fibonacci(long long input) {
  if (input < 2) return input;
  hpx::future<long long> n1 =
      hpx::async(fibonacci, input - 1);
  hpx::future<long long> n2 =
      hpx::async(fibonacci, input - 2);
  return n1.get() + n2.get();
}
\end{lstlisting}
  The HPX implementation of the Fibonacci algorithm can be seen in listing \ref{lst1:fibHPX}.
  In contrast to OpenMP, HPX works with futures to abstract results of function calls.
  Calling a function with \texttt{hpx::async} creates a task and immediately returns a future to continue the execution.
  Calling \texttt{get} on a future suspends the current thread until this future is returned.
  \\
 
  \paragraph{Merge Sort}
  The Sort algorithm of BOTS is slightly adjusted to a normal merge sort.
  It is a suitable use case, easy to implement and can also spawn a high number of tasks.
  Also in case of merge sort no cut off is implemented.
  \\
  
  \paragraph{Generic Algorithm}
  The aim of this algorithm is to enable task size adjustment and allow to define the number of tasks.
  
  The algorithm uses two vectors which sizes are defined at the building process.
  Suggested is a size of 1024 for each.
  First of all one vector is filled by randomized floating point numbers.
  To compare each run a deterministic seed for the random numbers is chosen.
  In each turn the element of a vector is equal to a calculation on the element of the other vector.
  The sinus function is calculated on the element before it is multiplied by ten and adjusted to an absolute value.
  This turns are repeated a defined number of times.
  After the last turn is executed, each elements of the last calculated vector are summed up.
  
  The task size can be adapted by parameters and defines how many vector elements are calculated by a task.
  The task size furthermore defines how many tasks are used per run.
  This number is equal to the vector size divided by the task size.
  As each turn depends on the execution of the previous turn, the number of dependencies can be increased by using more tasks per turn.
		
\subsection{Optimization}
	
	\cite{LaGrone.2011}
	--> task has an if clause evaluated to false will directly continue with the generated task --> depth-first
	--> untied tasks (balancing) vs tied (data locality)
	--> (maybe) use taskwait for less scheduling overhead --> depth-first
	--> cutoff, regulating task creation, if condition is not met execute tasks immediately and do not place them on the queue
		--> create new OMP\_TASK\_CREATE\_COND environment variable
		
    --> does the shared variable directive decrease the work for task creation???
    
    --> a nowait directive at the omp single
    
    \cite{MKlemm.2018}
 --> directives to optimize
 	-->taskyield - suspend the current task in favor of execution of a different task
 	--> if - in case it is false the task is executed immediately
 	--> mergeable - A task for which the data environment, is the same as that of its generating task region
 	--> final - force all child tasks to become final and included
 		--> if true --> child tasks become included --> means that the tasks are also executed by the parent task

  \cite{TheSTEARGroup.2020}
    - HPX --> Performance counters to identify bottlenecks
    - HPX exposes special API functions to allow one to create, manage and read the counter data
    - all performance counter instances have a unique name to access the counter


  \cite{hpxMP.2019}
  \cite{TheSTEARGroup.2020}
    - HPX supports 7 different thread scheduling policies:
      - priority scheduling /default: 1 high priority and 1 low priority queue Queue per OS thread
      - static priority scheduling: 1 low and 1 high prio queue per OS thread --> round robin used in queue
      - local scheduling: 1 queue per per OS thread
      - static scheduling: 1 queue per OS thread --> round robin
      - global scheduling: one shared queue
      - ABP scheduling: double ended lock-free queue per OS thread --> insert threads on top and steal threads from bottom
      - Hierarchy scheduling: tree of task items, each OS Thread traverses through to obtain new task item
      - Periodic priority scheduling: 1 queue of task items per OS thread, couple of high priority queues and one low priority queue
      