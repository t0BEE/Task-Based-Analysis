\section{Implementation \& Design}
\label{sec:implem}
  For this paper the Fibonacci and the Sort algorithm from BOTS are chosen to be implemented, as introduced in subsection \ref{subsec:BOTS}.
  Additionally, a generic algorithm is implemented.
  All three are implemented using OpenMP, HPX and in a sequential way.
  
  \paragraph{Fibonacci}
  The algorithm can spawn a high amount of tasks with small workload.
  The workload of forking and joining new threads might have the biggest impact when it comes to performance.
  
  The implementation is done without cut offs for the HPX and OpenMP versions.
  This means that every recursive call of the Fibonacci function creates a new task.
  \begin{lstlisting}[
  caption=Fibonacci OpenMP Implementation,
  label=lst1:fibOMP,
]
long long fibonacci(long long input) {
    if (input < 2 ) return input;
    long long x, y;
    #pragma omp task shared(x) firstprivate(input)
    x = fibonacci(input - 1);
    #pragma omp task shared(y) firstprivate(input)
    y = fibonacci(input - 2);
    #pragma omp taskwait
    return x + y;
}
\end{lstlisting}
  Listing \ref{lst1:fibOMP} shows a first implementation of the Fibonacci algorithm using OpenMP.
  It shows which directives are used.
  As explained in subsection \ref{subsec:OpenMP}, the \texttt{\#pragma omp task} directive creates a new task which may be executed.
  Using the \texttt{shared} and \texttt{firstprivate} clauses adjusts the variable scopes so that each task has its own instance of the parameter input.
  Variables \texttt{x} and \texttt{y} are shared which means no new instance is created.
  At the end the \texttt{\#pragma omp taskwait} directive tells the function to wait until all the child tasks finished their execution.
  
\begin{lstlisting}[
  caption=Fibonacci HPX Implementation,
  label=lst1:fibHPX,
]
long long fibonacci(long long input) {
  if (input < 2) return input;
  hpx::future<long long> n1 =
      hpx::async(fibonacci, input - 1);
  long long n2 = fibonacci(input - 2);
  return n1.get() + n2;
}
\end{lstlisting}
  The HPX implementation of the Fibonacci algorithm can be seen in listing \ref{lst1:fibHPX}.
  In contrast to OpenMP, HPX works with futures to abstract results of function calls.
  Calling a function with \texttt{hpx::async} creates a task and immediately returns a future to continue the execution.
  Calling \texttt{get} on a future suspends the current thread until this future is returned.
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.45\textwidth]{figures/fib_NoOp.png}
	\caption{Execution times of the Fibonacci algorithm}
	\label{fig:fib_NoOp}
\end{figure}

Figure \ref{fig:fib_NoOp} shows boxplots of 100 runs of the 22\textsuperscript{nd} Fibonacci number.
It illustrates the execution times of the sequential, OpenMP and HPX version.
The sequential and OpenMP implementation need the fewest time to complete, whereas the HPX version takes a lot more time.
This is due to the fact that HPX threads may be suspended in contrast to threads in OpenMP resulting in the HPX implementation having a bigger scheduling overhead.
Although the joining process at the end of a task region produces less overhead in HPX.
  \\
 
  \paragraph{Merge Sort}
  The Sort algorithm of BOTS is slightly adjusted to a normal merge sort.
  It is a suitable use case, easy to implement and can also spawn a high number of tasks.
  Also in case of merge sort no cut off is implemented.
  \begin{figure}[htbp]
	\centering
	\includegraphics[width=0.45\textwidth]{figures/sort_NoOp.png}
	\caption{Execution times of Merge Sort}
	\label{fig:sort_NoOp}
  \end{figure}
  The measurements of figure \ref{fig:sort_NoOp} are created on environment three and show the average of 100 runs.
  The merge sort is run on 10.000 random elements which are created in a deterministic way.
  Similar to Fibonacci, merge sort also creates a lot of tasks and HPX shows the slowest execution time.
  Sequential has the shortest average time and the OpenMP average times differ more significantly this time compared to the Fibonacci example. 
  \\
  
  \paragraph{Generic Algorithm}
  The aim of this algorithm is to enable task size adjustment and allow to define the number of tasks.
  
  The algorithm uses two vectors which sizes are defined at the building process.
  First of all one vector is filled by randomized floating point numbers.
  To compare each run a deterministic seed for the random numbers is chosen.
  In each turn the element of a vector is equal to a calculation on the element of the other vector.
  The sinus function is calculated on the element before it is multiplied by ten and adjusted to an absolute value.
  This turns are repeated a defined number of times.
  After the last turn is executed, all elements of the last calculated vector are summed up.
  
  The task size can be adapted by parameters and defines how many vector elements are calculated by a task.
  The task size furthermore defines how many tasks are used per run.
  This number is equal to the vector size divided by the task size.
  As each turn depends on the execution of the previous turn, the number of dependencies can be increased by using more tasks per turn.
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.45\textwidth]{figures/generic_NoOp.png}
	\caption{Execution times of the generic algorithm}
	\label{fig:gen_NoOp}
\end{figure}

Figure \ref{fig:gen_NoOp} shows the measured execution times for the generic algorithm in his three versions.
Again these are the average values of 100 runs on the system of environment three.
20 turns are made with a task size of 20 and the array size is 1,048,576.
It can be seen that the range of OpenMP times is quite big compared to the sequential and HPX version.
Additionally, the OpenMP version takes the longest time to complete the execution whereas HPX is the fastest of these three. 
